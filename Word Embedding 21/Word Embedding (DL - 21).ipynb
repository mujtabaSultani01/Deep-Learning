{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11262adc",
   "metadata": {},
   "source": [
    "## Word Embedding \n",
    "Text can be converted into numbers for training a model by using a technique called **vectorization**. Vectorization involves representing each word in the text as a numerical vector, which is then used as input to the machine learning model. Vectorization allows the model to understand the meaning of words in the text and make predictions based on the data. Vectorization can also be used to create features from the text, such as word count or frequency.\n",
    "\n",
    "**Word embedding** is a technique used in natural language processing (NLP) to represent words as numerical vectors. Word embeddings capture the semantic meaning of words by mapping them to a vector space. This allows the model to understand the context of words and make better predictions. **Word embeddings can be created using techniques such as word2vec or GloVe.**\n",
    "\n",
    "Word embedding is a process where words are represented as numerical vectors. This is done by mapping each word to a vector space, where the position of the vector corresponds to its semantic meaning. The process of creating word embeddings involves training an artificial neural network on a large dataset of text. The network learns the relationships between words and uses them to generate numerical vectors that capture their semantic meanings.\n",
    "\n",
    "**Word2vec** is a technique used in natural language processing (NLP) to represent words as numerical vectors. It uses a shallow two-layer neural network to learn the vector representations of words from large datasets. The model can then be used to generate numerical vectors that capture the semantic meaning of words in the dataset. Word2vec is commonly used for tasks such as sentiment analysis and text classification.\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)** is a technique used in natural language processing (NLP) to represent words as numerical vectors. It uses a weighted least squares regression model to learn the vector representations of words from large datasets. The model can then be used to generate numerical vectors that capture the semantic meaning of words in the dataset. GloVe is commonly used for tasks such as sentiment analysis and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6da65a",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1adbe",
   "metadata": {},
   "source": [
    "### Method 1: Unique Numbers\n",
    "Unique Number Technique (UNT) is a technique used in natural language processing (NLP) to represent text as numerical vectors. It uses a dictionary of numbers and maps each number in the text to its corresponding vector representation. UNT can be used to convert text into numerical vectors for use in machine learning models. It is commonly used for tasks such as sentiment analysis and text classification.\n",
    "* Converting each word into a specific unique number. \n",
    "* Follow the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718600dd",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815feb2",
   "metadata": {},
   "source": [
    "* **The issue with this approach is as follow:**\n",
    "    1. Numbers are random. They don't capture relationship between words. For example Axar and Dhone are both players and have similarity but we defined 2 and 7 random numbers for them. Next ashes is a tournament and Axar is a player but they have near distance with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ab3d0",
   "metadata": {},
   "source": [
    "### Method 2: One Hot Encoding\n",
    "- One-hot encoding is a technique used in machine learning to represent categorical data as numerical vectors. It encodes a category as a vector of zeros, with a single “1” indicating the presence of that category. One-hot encoding can be used to convert text into numerical vectors for use in machine learning models. It is commonly used for tasks such as sentiment analysis and text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847fea1",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47178a00",
   "metadata": {},
   "source": [
    "* **The issues with this approach is as follow:**\n",
    "    1. Doesn't capture relationship between words.\n",
    "    2. Computationally in-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566e1e2",
   "metadata": {},
   "source": [
    "### Method 3: Word Embedding\n",
    "- Word embedding is a technique used in natural language processing (NLP) to represent words as numerical vectors. Word embeddings capture the semantic meaning of words by mapping them to a vector space. This allows the model to understand the context of words and make better predictions. Word embeddings can be created using techniques such as word2vec or GloVe.\n",
    "- Word Embedding allows you to capture the relationship (similarities) between two words.\n",
    "- Follow the bellow images to clear the idea how word embedding word:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1699182",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538eb0d4",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438532a",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443ea3f",
   "metadata": {},
   "source": [
    "<img src = \"img6.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db21d16",
   "metadata": {},
   "source": [
    "So till now we covered that word embedding is a better technique to represent text as numbers. Now the question is how we should come out with the features? It's looking to be a very challenging task, but the good thing is that Neural Network can compute these features for us. The features aren't hand crafted (Embedding aren't hand crafted.), They are derived while training the neural network.  \n",
    "- **There are two techniques to compute word embedding:**\n",
    "    1. Supervised Learning\n",
    "    2. Self Supervised Learning (Word2Vec & Glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb7657",
   "metadata": {},
   "source": [
    "#### Supervised Learning technique\n",
    "In supervised learning technique we take an NLP problem and we try to solve the NLP problem, so as side effect we get word embedding. So based on general concept of Neural Network the feature vectors are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe205c",
   "metadata": {},
   "source": [
    "* Let's practically implement **Supervised Learning technique** for word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833976ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries:\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf50b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here we classifying the restaurant reviews.\n",
    "# Total we have total 10 reviews.\n",
    "reviews = ['nice food',\n",
    "        'amazing restaurant',\n",
    "        'too good',\n",
    "        'just loved it!',\n",
    "        'will go again',\n",
    "        'horrible food',\n",
    "        'never go there',\n",
    "        'poor service',\n",
    "        'poor quality',\n",
    "        'needs improvement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e022baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels of reviews are shown bellow, the first five reviews are positive and the last five reviews are negative:\n",
    "sentiment = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b47319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So first we want to convert the reviews into numbers using One Hot Encoding. \n",
    "# Here we use one_hot method which take the review and your vocabulary size. \n",
    "# So if we say 30 for the vocabulary size, it will gibe the unique number for each word (the number is between 1 and 30).\n",
    "# If we say 500, then the assigned numbers will be between 1 and 500.\n",
    "# So One Hot Encoding is giving the fix (unique) number and internally the layers will convert it into 0,0,1,0,1 and so on.\n",
    "one_hot(\"amazing restaurant\", 30)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7073690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 25],\n",
       " [24, 6],\n",
       " [25, 1],\n",
       " [3, 25, 24],\n",
       " [19, 15, 23],\n",
       " [22, 25],\n",
       " [21, 15, 7],\n",
       " [6, 25],\n",
       " [6, 8],\n",
       " [27, 23]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to encode all the reviews into one hot encoding. So first we assign the vocabulary size as 30.\n",
    "# So here we did simple list comprehension, we go through all the reviews, and for each review we created one hot encoded \n",
    "# vectors.\n",
    "voc_size = 30;\n",
    "encoded_reviews = [one_hot(d, voc_size) for d in reviews]\n",
    "#print(encoded_reviews)\n",
    "encoded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c8ea21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 25,  0],\n",
       "       [24,  6,  0],\n",
       "       [25,  1,  0],\n",
       "       [ 3, 25, 24],\n",
       "       [19, 15, 23],\n",
       "       [22, 25,  0],\n",
       "       [21, 15,  7],\n",
       "       [ 6, 25,  0],\n",
       "       [ 6,  8,  0],\n",
       "       [27, 23,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So here some reviews have two words and some reviews have three words. So for the two words reviews we need to do padding.\n",
    "# Padding means, we add zeroes instead of the third words into two words reviews.\n",
    "# So first we define 'max_lenght' which may be 3 or 4.\n",
    "# Then we use keras 'pad_sequence' method, and we supply all the encoded sequences to the function with addition to max_\n",
    "# lenght. paddin = 'post' means tht pad the reviews towards the end. Toward the end we'll get the zeroes.\n",
    "max_length = 3\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "padded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51272932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now our padded reviews have equal size. Every vector size is three.\n",
    "# The next step is embedded vectors size. Let's we have 5 embedded vector size. By embeded vector size means the number of \n",
    "# features which we'll have for each word.\n",
    "# Next we create NN model. And our first layer will be embedding layer. For embedding layer we use 'Embedding' class.\n",
    "# The way embedding class work is, it takes couple of arguments: the first argument is vocabulary size and the second argum-\n",
    "# ent is the embeded vector size and we also supply the max lenght of the reviews and finally we give it a name to use it l-\n",
    "# ater. The second layer is (when we get embedded vectors from the embedded layer, the next stpe (layer) is to flatten the -\n",
    "# vectors to create a general vectors.\n",
    "# The next layer is One neuron sigmoid activation function (dense layer with sigmoid activation function).\n",
    "\n",
    "embeded_vector_size = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d83c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the next stpe is to define X and Y. So to keep thing simple, our X is padded reviews and Y is sentiment.\n",
    "X = padded_reviews\n",
    "y = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bba32f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 5)              150       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 15)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166\n",
      "Trainable params: 166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next we compile the model and print the model summary:\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecf7b0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18354a4b1c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to train the model for epochs 50.\n",
    "model.fit(X, y, epochs = 100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "949f9d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5248 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now once the model is trined, let's check the accuracy.\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf3cc4",
   "metadata": {},
   "source": [
    "* **So as the dataset is very small, our accuracy is 100%.**\n",
    "* So **Word Embedding** is nothing but those parameters which are in your neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59115e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06113086, -0.1187233 , -0.09578586, -0.11541521,  0.08865828],\n",
       "       [ 0.04690395, -0.12556437, -0.12741852,  0.08548369,  0.14064033],\n",
       "       [-0.00943569, -0.02478651, -0.04124054,  0.00321704,  0.04346119],\n",
       "       [-0.05460171, -0.10659345, -0.17304026,  0.12057173,  0.08760702],\n",
       "       [-0.03408168, -0.0275035 , -0.004375  ,  0.04581155, -0.01392312],\n",
       "       [ 0.00162426,  0.04818442, -0.03545678,  0.03306409, -0.04704807],\n",
       "       [ 0.05532457,  0.12023149, -0.07434893, -0.10334461, -0.13416561],\n",
       "       [ 0.08107556, -0.14074881, -0.13753182, -0.09706683,  0.08575559],\n",
       "       [-0.11736707, -0.13170387,  0.11831835,  0.08755741, -0.0598025 ],\n",
       "       [-0.04500035,  0.03035133, -0.03501581, -0.00991637,  0.04487015],\n",
       "       [-0.04602529,  0.00416138, -0.01348054, -0.02405175,  0.03791169],\n",
       "       [-0.01787271, -0.02889364, -0.00493195,  0.00060501, -0.03145068],\n",
       "       [ 0.02629424,  0.0340098 , -0.04178872,  0.00815178,  0.03532988],\n",
       "       [ 0.01060029, -0.0106612 , -0.01995409,  0.03878976,  0.0099291 ],\n",
       "       [ 0.02787327,  0.03578937, -0.0321606 ,  0.0441174 , -0.03256667],\n",
       "       [-0.02344234, -0.04147458,  0.04366433,  0.08030362, -0.05286796],\n",
       "       [ 0.04995439,  0.01842308,  0.03623569,  0.04036515, -0.03694103],\n",
       "       [ 0.03017614, -0.01061919, -0.01538807,  0.03610169, -0.04400907],\n",
       "       [ 0.04223747,  0.02342942, -0.03557643,  0.00888007,  0.00522916],\n",
       "       [-0.15166645, -0.11359662, -0.11237905,  0.13687892,  0.08714917],\n",
       "       [-0.00268   , -0.01322675, -0.04529003,  0.01441062, -0.03101277],\n",
       "       [ 0.11440857,  0.07582545,  0.13378976, -0.11589037, -0.05619667],\n",
       "       [ 0.13873571,  0.11095498,  0.12287724, -0.07278381, -0.1230325 ],\n",
       "       [-0.14478625,  0.04988728,  0.11613297,  0.0900894 , -0.06366667],\n",
       "       [-0.0820071 ,  0.04073719,  0.03709897,  0.12229957,  0.10632784],\n",
       "       [-0.12995356, -0.08347256, -0.1394918 ,  0.1391611 ,  0.14785933],\n",
       "       [-0.03763088, -0.03855506,  0.02509357, -0.04863107, -0.011338  ],\n",
       "       [ 0.1418041 ,  0.07358938,  0.17252718, -0.0512221 , -0.13786322],\n",
       "       [-0.04646627,  0.03800987,  0.01708423,  0.02191141,  0.01107373],\n",
       "       [-0.02364287,  0.0077268 ,  0.02545765, -0.00771631,  0.03557619]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use a method called 'get_layer' which takes layer_name and will return all the weights. So these are the weights \n",
    "# which we were looking for.\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "255d76b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we check the lenght of these weights, it should be 30, because our vocabulary size was 30.\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "288f5546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04690395, -0.12556437, -0.12741852,  0.08548369,  0.14064033],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's check the weights for some words. If we check it for words 'nice' and 'amazing', the generated vectors will be \n",
    "# almost similar because these words are looking to have similar meaning. \n",
    "# Weights for word 'nice'. // we give 1 for word nice, because when we were doing one hot encoding, the number '1' was \n",
    "                          # assigned for word 'nice'.\n",
    "weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae46f4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0820071 ,  0.04073719,  0.03709897,  0.12229957,  0.10632784],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights for word 'amazing':\n",
    "weights[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bebdc",
   "metadata": {},
   "source": [
    "* So as our dataset is very very small, the weights of these two words 'nice' and 'amazing' are looking different. If we have big dataset, then surly the weights of these words be similar and we can compute the cosine similarity.\n",
    "* So the way Keras Embedding layer works, it compute the embedding on different NLP tasks.\n",
    "* The other way of computing embedding is, we can save the computted embedding and save it in a file and later on when we want to perform a different type of tasks, we can load the saved embeddings from that file into the embedding layer and use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2a5f1",
   "metadata": {},
   "source": [
    "#### To know more about word embedding (using saved weights ...) you can read the following article:\n",
    "    https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
