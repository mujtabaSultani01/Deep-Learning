{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11262adc",
   "metadata": {},
   "source": [
    "## Word Embedding \n",
    "Text can be converted into numbers for training a model by using a technique called **vectorization**. Vectorization involves representing each word in the text as a numerical vector, which is then used as input to the machine learning model. Vectorization allows the model to understand the meaning of words in the text and make predictions based on the data. Vectorization can also be used to create features from the text, such as word count or frequency.\n",
    "\n",
    "**Word embedding** is a technique used in natural language processing (NLP) to represent words as numerical vectors. Word embeddings capture the semantic meaning of words by mapping them to a vector space. This allows the model to understand the context of words and make better predictions. **Word embeddings can be created using techniques such as word2vec or GloVe.**\n",
    "\n",
    "Word embedding is a process where words are represented as numerical vectors. This is done by mapping each word to a vector space, where the position of the vector corresponds to its semantic meaning. The process of creating word embeddings involves training an artificial neural network on a large dataset of text. The network learns the relationships between words and uses them to generate numerical vectors that capture their semantic meanings.\n",
    "\n",
    "**Word2vec** is a technique used in natural language processing (NLP) to represent words as numerical vectors. It uses a shallow two-layer neural network to learn the vector representations of words from large datasets. The model can then be used to generate numerical vectors that capture the semantic meaning of words in the dataset. Word2vec is commonly used for tasks such as sentiment analysis and text classification.\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)** is a technique used in natural language processing (NLP) to represent words as numerical vectors. It uses a weighted least squares regression model to learn the vector representations of words from large datasets. The model can then be used to generate numerical vectors that capture the semantic meaning of words in the dataset. GloVe is commonly used for tasks such as sentiment analysis and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6da65a",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1adbe",
   "metadata": {},
   "source": [
    "### Method 1: Unique Numbers\n",
    "Unique Number Technique (UNT) is a technique used in natural language processing (NLP) to represent text as numerical vectors. It uses a dictionary of numbers and maps each number in the text to its corresponding vector representation. UNT can be used to convert text into numerical vectors for use in machine learning models. It is commonly used for tasks such as sentiment analysis and text classification.\n",
    "* Converting each word into a specific unique number. \n",
    "* Follow the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718600dd",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815feb2",
   "metadata": {},
   "source": [
    "* **The issue with this approach is as follow:**\n",
    "    1. Numbers are random. They don't capture relationship between words. For example Axar and Dhone are both players and have similarity but we defined 2 and 7 random numbers for them. Next ashes is a tournament and Axar is a player but they have near distance with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ab3d0",
   "metadata": {},
   "source": [
    "### Method 2: One Hot Encoding\n",
    "- One-hot encoding is a technique used in machine learning to represent categorical data as numerical vectors. It encodes a category as a vector of zeros, with a single “1” indicating the presence of that category. One-hot encoding can be used to convert text into numerical vectors for use in machine learning models. It is commonly used for tasks such as sentiment analysis and text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847fea1",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47178a00",
   "metadata": {},
   "source": [
    "* **The issues with this approach is as follow:**\n",
    "    1. Doesn't capture relationship between words.\n",
    "    2. Computationally in-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566e1e2",
   "metadata": {},
   "source": [
    "### Method 3: Word Embedding\n",
    "- Word embedding is a technique used in natural language processing (NLP) to represent words as numerical vectors. Word embeddings capture the semantic meaning of words by mapping them to a vector space. This allows the model to understand the context of words and make better predictions. Word embeddings can be created using techniques such as word2vec or GloVe.\n",
    "- Word Embedding allows you to capture the relationship (similarities) between two words.\n",
    "- Follow the bellow images to clear the idea how word embedding word:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1699182",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538eb0d4",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438532a",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443ea3f",
   "metadata": {},
   "source": [
    "<img src = \"img6.jpg\" width = \"900px\" height = \"600px\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
