{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbcf788",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec is a technique in computer science that allows you to do mathematics with the words. For example if you give the equation \"King - man + woman\" to the computer, computer will tell you the answer is \"Queen\". So now the question is how computer can do this? So computer don't understand text, it understand numbers. So if there is a word to represent king in a number such that it can accurately represent the meaning of the word 'King', now the number can't be one number, so we need to have a set of numbers and in mathematics set of numbers are called vectors. So let's think now that how to represent word 'King' into a vector (which is just a bunch of numbers) such that it can represent the meaning of word 'King' accurately. So now if we think there are different ways of representing the word 'King'. For example 'King' has authority, 'King' is rich, 'King' has a gender of male, and so does 'King' has a population? no generally countries or cities have populations, so this is false about representing 'King'. So the vector generated from word 'King' will be [authority = 1, rich = 1, population = 0, gender = 1] =>\n",
    "So the vector will be: [1 1 0 1]. For other words we'll have the same generated vectors. So once we have such vectors for all the words in vocabulary, then we're able to do math easilly. See the bellow example image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a9ff1",
   "metadata": {},
   "source": [
    "<img src = \"img.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c025141",
   "metadata": {},
   "source": [
    "* So here in the above image, 'authority', 'event', 'has tail', 'rich' and 'gender' are called features. The vectors generatd for King, Man, Woman, Queen, Battle, and Horse are called feature vectors.\n",
    "* Generatig features vectors manually is very very difficult, so for generating these feature vectors we use neural networks, and neural network will authomatically learn these features.\n",
    "* Generating these feature vectors are also called word embedding.\n",
    "* The wonderful thing about word embedding is, that we don't know in vector 'King' the first 1 is 'authority', it works magically.\n",
    "* **See the fake problem of missing words in a sentence as whown in the following images:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1dfc7",
   "metadata": {},
   "source": [
    "<img src = \"img1.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c587d0a",
   "metadata": {},
   "source": [
    "<img src = \"img2.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46345c99",
   "metadata": {},
   "source": [
    "* So now as shown in the following images, if I take a window of three words, and I say if there is a words 'lived' and 'a', so then I can preict that there is a word 'there'. So here we're taking the 2nd and 3rd words and we're predicting the 3rd word.\n",
    "* So these are our training samples. (We move the window of three words over the paragraph & generate all train samples. As result the generated training samples becomes the training set for the neural network.):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58505d",
   "metadata": {},
   "source": [
    "<img src = \"img3.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324234be",
   "metadata": {},
   "source": [
    "<img src = \"img4.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72580f6f",
   "metadata": {},
   "source": [
    "<img src = \"img5.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c7517",
   "metadata": {},
   "source": [
    "* In the training samples, the words in left side are X and on the right side is Y. You feed the words X and predict the word Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649ad92",
   "metadata": {},
   "source": [
    "* Now to train our neural network using each of these samples, first let's we have words 'ordered' and 'his', based on these two words we want to predict the word 'king'. So the input layer will have One Hot Encoded vectors. If we have 5000 words in the vocabulary, then in input layer there will be a vector which has 5000 size and only one word will be one and the remaining will be zero. For exaple if the word is 'ordered' then in the vector the value of word 'ordered' will be 1 and the remaining numbers will be zero. Same for word 'his'. \n",
    "* Vocabulary means unique word in your text corpus.\n",
    "* In the hidden layer we have 4 neurons, and these 4 neuron are the size of the embedding vectors. The size may be different, there is no any rule, just trail and error.\n",
    "* In the output layer we'll have a 5000 size vector. So when we feed these training samples into neural network, what happen is, first the edges will have random values (weights), based on these random weights the output will be generated which might be wrong most likely. So we compare the actual output Y with the predicted output Y'. We take a loss which is a difference between the actual output and the predicted output and we back propagate. When we back propagate, essentially what we do is audjusting the weights, then we take a second sample, third sample ... we take all 5000 or 100000 samples & we train the model in such a way that if our input is 'ordered' and 'his' then the output should be 'King'.\n",
    "* **The process is shown in images bellow:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998480c",
   "metadata": {},
   "source": [
    "<img src = \"img6.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6b61a",
   "metadata": {},
   "source": [
    "<img src = \"img7.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba611b8d",
   "metadata": {},
   "source": [
    "* **The second sample is (which is window of size 3):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59700b",
   "metadata": {},
   "source": [
    "<img src = \"img8.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32219a9",
   "metadata": {},
   "source": [
    "* **The 3rd sample:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db6879",
   "metadata": {},
   "source": [
    "<img src = \"img9.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197bb19",
   "metadata": {},
   "source": [
    "* So when we feed let's say 1 milion samples and also we run 100 epochs, so our neural network will be trained. At that point the word vector for 'King' would be the weights which is shown in the following image: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2e9f7",
   "metadata": {},
   "source": [
    "<img src = \"img10.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542d960",
   "metadata": {},
   "source": [
    "* Those weights are nothing but a trained word vector. And these vectors will be very similar with the vectors of word 'emperor' because the input is same. See the bellow image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b3933",
   "metadata": {},
   "source": [
    "<img src = \"img11.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612c9ab",
   "metadata": {},
   "source": [
    "* So the upper approach is called **Continues Bag Of Words (CBOW).**\n",
    "* In CBOW based on context we predict the target word.\n",
    "* There is 2nd methodology which is called **Skip Gram**. in skip gram we do reverse, we have a target word and based on that we predict the context words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c8a09",
   "metadata": {},
   "source": [
    "<img src = \"img12.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8194c51",
   "metadata": {},
   "source": [
    "* To **summarize,** Word2Vec is not  single method, but it can use one of the two techniques which is **CBOW** and **Skip Gram** to learn word embedding.\n",
    "* Simply **Word2Vec** means converting word into vectors.\n",
    "* The **Skip Gram** working mechanism is shown in bellow images:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d1262",
   "metadata": {},
   "source": [
    "<img src = \"img13.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe239bdc",
   "metadata": {},
   "source": [
    "<img src = \"img14.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9ee9e",
   "metadata": {},
   "source": [
    "<img src = \"img15.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a793cf9",
   "metadata": {},
   "source": [
    "* So when you're using **Skip Gram**, the word embedding (weights) is a layer between the input layer and a hidden layer, but when you're using **CBOW**, the word embedding (weights) is between hidden layer and the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2fac4",
   "metadata": {},
   "source": [
    "<img src = \"img16.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8811fd1",
   "metadata": {},
   "source": [
    "<img src = \"img17.png\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107dd61",
   "metadata": {},
   "source": [
    "### Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15b14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you need to install 'gensim' library, but if you install Anaconda, then it's installed in your computer.\n",
    "# Next you need to install another model called 'python-Levenshtein' which is not comming with Anaconda.\n",
    "# !pip install gensim\n",
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2913b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first import gensim and pandas.\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34156896",
   "metadata": {},
   "source": [
    "#### Reading and Exploring the Dataset\n",
    "The dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas.\n",
    "* Here we use gensim library which is an NLP library for python, and it's very easy to use the syntax is very easy compared to tensorflow, so thats why we want to use it to build a **Word2Vec Model.**\n",
    "\n",
    "Link to the Dataset: http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1581f517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28238</th>\n",
       "      <td>A1H9ZV1NWGJNG3</td>\n",
       "      <td>B0042UQLM0</td>\n",
       "      <td>Mike</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I had to get this use my headphones on my xbox...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great</td>\n",
       "      <td>1390089600</td>\n",
       "      <td>01 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143844</th>\n",
       "      <td>A18P98QS2M3K6Z</td>\n",
       "      <td>B00AR4M04S</td>\n",
       "      <td>Jim Besso</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These worked great -- at first.  After a coupl...</td>\n",
       "      <td>3</td>\n",
       "      <td>Not up to par</td>\n",
       "      <td>1403654400</td>\n",
       "      <td>06 25, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28173</th>\n",
       "      <td>A1RP5VRESSVIHJ</td>\n",
       "      <td>B0042TY68C</td>\n",
       "      <td>reazon i am needing help</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>i ordered two of these. they do NOT provide mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>i wasted my money</td>\n",
       "      <td>1373068800</td>\n",
       "      <td>07 6, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158817</th>\n",
       "      <td>A2LE5IS9W9OWXC</td>\n",
       "      <td>B00CB6X6Y8</td>\n",
       "      <td>Imad Ali Syed</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This product works perfectly for me in my Niss...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Service</td>\n",
       "      <td>1397433600</td>\n",
       "      <td>04 14, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699</th>\n",
       "      <td>A3919EID796R4N</td>\n",
       "      <td>B001DDG2FA</td>\n",
       "      <td>Sherry N. Efferson</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Much better price than cell service carrier - ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Cell phone batteries</td>\n",
       "      <td>1225670400</td>\n",
       "      <td>11 3, 2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin              reviewerName helpful  \\\n",
       "28238   A1H9ZV1NWGJNG3  B0042UQLM0                      Mike  [0, 0]   \n",
       "143844  A18P98QS2M3K6Z  B00AR4M04S                 Jim Besso  [0, 0]   \n",
       "28173   A1RP5VRESSVIHJ  B0042TY68C  reazon i am needing help  [0, 0]   \n",
       "158817  A2LE5IS9W9OWXC  B00CB6X6Y8             Imad Ali Syed  [0, 0]   \n",
       "6699    A3919EID796R4N  B001DDG2FA        Sherry N. Efferson  [0, 0]   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "28238   I had to get this use my headphones on my xbox...        5   \n",
       "143844  These worked great -- at first.  After a coupl...        3   \n",
       "28173   i ordered two of these. they do NOT provide mo...        1   \n",
       "158817  This product works perfectly for me in my Niss...        5   \n",
       "6699    Much better price than cell service carrier - ...        5   \n",
       "\n",
       "                     summary  unixReviewTime   reviewTime  \n",
       "28238                  Great      1390089600  01 19, 2014  \n",
       "143844         Not up to par      1403654400  06 25, 2014  \n",
       "28173      i wasted my money      1373068800   07 6, 2013  \n",
       "158817         Great Service      1397433600  04 14, 2014  \n",
       "6699    Cell phone batteries      1225670400   11 3, 2008  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so let's read the dataset. The dataset has json reviews and pandas has the power to read json file.\n",
    "# 'Lines=True' means each json review will be read in single line.\n",
    "df = pd.read_json(\"Cell_Phones_and_Accessories_5.json\", lines=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b76672d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194439, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we want to traine a word2Vec model using only a 'reviewText' column.\n",
    "# To get an idea how many records are there in the dataset we do shape:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ae6070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check a single review of 'reviewText' column:\n",
    "df.reviewText[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84109c8",
   "metadata": {},
   "source": [
    "### Simple Preprocessing & Tokenization\n",
    "* The first thing to do for any data science task is to clean the data. For NLP, we apply various processing like converting all the words to lower case, trimming spaces, removing punctuations. This is something we will do over here too.\n",
    "\n",
    "* Additionally, we can also remove stop words like 'and', 'or', 'is', 'the', 'a', 'an' and convert words to their root forms like 'running' to 'run'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62fe928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they',\n",
       " 'look',\n",
       " 'good',\n",
       " 'and',\n",
       " 'stick',\n",
       " 'good',\n",
       " 'just',\n",
       " 'don',\n",
       " 'like',\n",
       " 'the',\n",
       " 'rounded',\n",
       " 'shape',\n",
       " 'because',\n",
       " 'was',\n",
       " 'always',\n",
       " 'bumping',\n",
       " 'it',\n",
       " 'and',\n",
       " 'siri',\n",
       " 'kept',\n",
       " 'popping',\n",
       " 'up',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'irritating',\n",
       " 'just',\n",
       " 'won',\n",
       " 'buy',\n",
       " 'product',\n",
       " 'like',\n",
       " 'this',\n",
       " 'again']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first step in training word2Vec is to do pre-processing, because the text have stop words and we want to convert the \n",
    "# words into lower case we remove the spaces, and we remove the punctuation marks and so on...\n",
    "# So these thing can be done by a function in gensim called 'gensim.util.simple_preprocess'. So this function will simply \n",
    "# preprocess the text.\n",
    "# Let's pass a single review to the function:\n",
    "gensim.utils.simple_preprocess(\"They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4989b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130150    [this, is, nice, case, for, the, price, you, g...\n",
       "74315     [ordered, this, as, part, of, our, back, up, s...\n",
       "57341     [this, battery, works, well, for, me, charges,...\n",
       "164356    [it, great, to, keep, your, phone, covered, bu...\n",
       "171456    [was, really, blurry, so, was, really, disappo...\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So as we see the function tokenize the review, covert the capital letters to small letters, remove punctuation marks, and\n",
    "# also removed the stop words such as I.\n",
    "# So now we supply the entire column to the function by using apply function. As result it will return you numpy series.\n",
    "review_text = df.reviewText.apply(gensim.utils.simple_preprocess)\n",
    "review_text.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf5e1d",
   "metadata": {},
   "source": [
    "#### Training the Word2Vec Model\n",
    "* Train the model for reviews. Use a window of size 10 i.e. 10 words before the present word and 10 words ahead. A sentence with at least 2 words should only be considered, configure this using min_count parameter.\n",
    "* Workers define how many CPU threads to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e557a",
   "metadata": {},
   "source": [
    "#### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a320ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So it creates a new pandas series, and each object in the series is a list and each list has tokenized words.\n",
    "# So next we initialize gensim model. Gensim is an NLP library and Word2Vec class come with this library which takes couple\n",
    "# of arguments. The first parameter is window size (if we assign 10, it means 10 words before the target word and 10 words \n",
    "# after the target word.). The 2nd argument is 'min_count', so mean_count is basically, if you have a sentence which has \n",
    "# only one word then we don't use that sentence, so mean_count say that at least n words should be in a sentence in order to\n",
    "# be considered in the training process. The next argument is 'workers' which means how many threads of CPU do you want to \n",
    "# use to train the model?\n",
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01887a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to build a vocabulary, building vocabulary means building a unique list of words.\n",
    "# So for that we called a function 'build_vocab()' and we supply the tokenize text. \n",
    "model.build_vocab(review_text, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "194dca5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default the number of epochs are set to 5:\n",
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8278f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61503356, 83868975)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we want to perform the actual training by using 'train()' function and first we supply the review text and 2nd we \n",
    "# supply total_examples which is the total number of reviews in the corpus and the 3rd argument is the number of epochs.\n",
    "model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a8d6a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194439"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eedfb5",
   "metadata": {},
   "source": [
    "#### Save the Model\n",
    "* Save the model so that it can be reused in other applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db25c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the model is trained. We want to first save the model in file. Because normally we train the model then we save it in\n",
    "# the file and then we use the pre-trained model.\n",
    "# Once the model is saved then we can take it and store it in the cloud, then accroding to our NLP needs we can use it.\n",
    "model.save(\"./word2vec-amazon-cell-accessories-reviews-short.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820d372",
   "metadata": {},
   "source": [
    "#### Finding Similar Words and Similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e06a518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.6764117479324341),\n",
       " ('shabby', 0.6534987092018127),\n",
       " ('horrible', 0.6188998222351074),\n",
       " ('good', 0.576382040977478),\n",
       " ('awful', 0.5688327550888062),\n",
       " ('okay', 0.5432535409927368),\n",
       " ('mad', 0.5354875922203064),\n",
       " ('cheap', 0.5241401791572571),\n",
       " ('disappointing', 0.5223191380500793),\n",
       " ('keen', 0.520203709602356)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next step is to experiment the model. The way we do that is to call 'Word2Vec = wv' and it has the function 'most_si-\n",
    "# milar()'.\n",
    "# So if we pass a word to the function, it will return all the words  which are similar to the supplied word with similarity\n",
    "# scores.\n",
    "model.wv.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7758821c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50951564"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we can print the cosine similarity scores of the model between two words:\n",
    "model.wv.similarity(w1=\"cheap\", w2=\"inexpensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8a9dc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7796989"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try two other words:\n",
    "model.wv.similarity(w1=\"great\", w2=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c73d53da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"good\", w2=\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f9fef",
   "metadata": {},
   "source": [
    "#### Further Reading\n",
    "* You can read about gensim more at https://radimrehurek.com/gensim/models/word2vec.html\n",
    "* Explore other Datasets related to Amazon Reviews: http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963641c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Train a word2vec model on the **Sports & Outdoors Reviews Dataset** Once you train a model on this, find the words most similar to 'awful' and find similarities between the following word tuples: ('good', 'great'), ('slow','steady')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4c7a4a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34796</th>\n",
       "      <td>A1KHNJ93BP4ELV</td>\n",
       "      <td>B000ENMNCQ</td>\n",
       "      <td>Nikola Edison</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I wear a swim cap over ear plugs as I am intol...</td>\n",
       "      <td>4</td>\n",
       "      <td>Better than average.</td>\n",
       "      <td>1371254400</td>\n",
       "      <td>06 15, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84954</th>\n",
       "      <td>A21J6AKQZ0V6C5</td>\n",
       "      <td>B001298HEE</td>\n",
       "      <td>Kinsykins</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Just spray it on and the gunk literally runs o...</td>\n",
       "      <td>5</td>\n",
       "      <td>It Works</td>\n",
       "      <td>1371600000</td>\n",
       "      <td>06 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167904</th>\n",
       "      <td>A1BUDU8SNJ0I9W</td>\n",
       "      <td>B002TUSJWA</td>\n",
       "      <td>J. Wu \"JackandBlood\"</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>For the price this bi-pod does very well.  Mat...</td>\n",
       "      <td>5</td>\n",
       "      <td>Cant go wrong for the price</td>\n",
       "      <td>1291939200</td>\n",
       "      <td>12 10, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26145</th>\n",
       "      <td>A66DME0HFC10K</td>\n",
       "      <td>B000AXAO3U</td>\n",
       "      <td>An American Soldier \"Six\"</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>Absolutely all it's said to be.  Keeps weapons...</td>\n",
       "      <td>5</td>\n",
       "      <td>The Real Deal in Weapon Protection</td>\n",
       "      <td>1259884800</td>\n",
       "      <td>12 4, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28926</th>\n",
       "      <td>AFECGAER92D8C</td>\n",
       "      <td>B000BRQVH8</td>\n",
       "      <td>Rational</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>These seem well made, although, having had the...</td>\n",
       "      <td>3</td>\n",
       "      <td>Nicely made</td>\n",
       "      <td>1337731200</td>\n",
       "      <td>05 23, 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin               reviewerName helpful  \\\n",
       "34796   A1KHNJ93BP4ELV  B000ENMNCQ              Nikola Edison  [0, 0]   \n",
       "84954   A21J6AKQZ0V6C5  B001298HEE                  Kinsykins  [0, 0]   \n",
       "167904  A1BUDU8SNJ0I9W  B002TUSJWA       J. Wu \"JackandBlood\"  [1, 1]   \n",
       "26145    A66DME0HFC10K  B000AXAO3U  An American Soldier \"Six\"  [3, 3]   \n",
       "28926    AFECGAER92D8C  B000BRQVH8                   Rational  [1, 3]   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "34796   I wear a swim cap over ear plugs as I am intol...        4   \n",
       "84954   Just spray it on and the gunk literally runs o...        5   \n",
       "167904  For the price this bi-pod does very well.  Mat...        5   \n",
       "26145   Absolutely all it's said to be.  Keeps weapons...        5   \n",
       "28926   These seem well made, although, having had the...        3   \n",
       "\n",
       "                                   summary  unixReviewTime   reviewTime  \n",
       "34796                 Better than average.      1371254400  06 15, 2013  \n",
       "84954                             It Works      1371600000  06 19, 2013  \n",
       "167904         Cant go wrong for the price      1291939200  12 10, 2010  \n",
       "26145   The Real Deal in Weapon Protection      1259884800   12 4, 2009  \n",
       "28926                          Nicely made      1337731200  05 23, 2012  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first load the dataset:\n",
    "dfe = pd.read_json(\"Sports_and_Outdoors_5.json\", lines=True)\n",
    "dfe.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6553c074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296337, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the shape of dataset:\n",
    "dfe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8fd74c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243119    [it, works, great, no, directions, so, it, was...\n",
       "111172    [these, small, projectiles, are, perfect, trai...\n",
       "73785     [use, this, in, conjunction, with, my, battle,...\n",
       "180745    [these, things, are, nice, and, tight, but, no...\n",
       "272910    [compact, easy, to, use, and, relatively, infi...\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next step is to pre-process the text:\n",
    "review_text_ex = dfe.reviewText.apply(gensim.utils.simple_preprocess)\n",
    "review_text_ex.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d14f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we initialize the model:\n",
    "model_ex = gensim.models.Word2Vec(\n",
    "    window=8,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf7319a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to build a vocabulary:\n",
    "model_ex.build_vocab(review_text_ex, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f20dbe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91341518, 121496535)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next step is perform the actual training:\n",
    "model_ex.train(review_text_ex, total_examples=model_ex.corpus_count, epochs=model_ex.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0425db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the model is trained,so we store it in a file:\n",
    "model_ex.save(\"./word2vec-Sports_and_Outdoors_5.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "025f6e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outstanding', 0.9000451564788818),\n",
       " ('exceptional', 0.8672378063201904),\n",
       " ('incredible', 0.7825648188591003),\n",
       " ('excellant', 0.7681642770767212),\n",
       " ('awesome', 0.7564525604248047),\n",
       " ('excelent', 0.7422342896461487),\n",
       " ('superb', 0.7302507758140564),\n",
       " ('fantastic', 0.7212028503417969),\n",
       " ('amazing', 0.7166146039962769),\n",
       " ('unbeatable', 0.7148966193199158)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can expriment the model:\n",
    "model_ex.wv.most_similar(\"excellent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da215188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.530342"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we can print the cosine similarity scores of the model between two words:\n",
    "model_ex.wv.similarity(w1=\"cheap\", w2=\"inexpensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1816184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80471194"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"superb\", w2=\"excellent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ff994e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"nice\", w2=\"nice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e199e7f",
   "metadata": {},
   "source": [
    "* **Thats were all for this notebook.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
