{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa69a662",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec is a technique in computer science that allows you to do mathematics with the words. For example if you give the equation \"King - man + woman\" to the computer, computer will tell you the answer is \"Queen\". So now the question is how computer can do this? So computer don't understand text, it understand numbers. So if there is a word to represent king in a number such that it can accurately represent the meaning of the word 'King', now the number can't be one number, so we need to have a set of numbers and in mathematics set of numbers are called vectors. So let's think now that how to represent word 'King' into a vector (which is just a bunch of numbers) such that it can represent the meaning of word 'King' accurately. So now if we think there are different ways of representing the word 'King'. For example 'King' has authority, 'King' is rich, 'King' has a gender of male, and so does 'King' has a population? no generally countries or cities have populations, so this is false about representing 'King'. So the vector generated from word 'King' will be [authority = 1, rich = 1, population = 0, gender = 1] =>\n",
    "So the vector will be: [1 1 0 1]. For other words we'll have the same generated vectors. So once we have such vectors for all the words in vocabulary, then we're able to do math easilly. See the bellow example image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b84be2",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cfb53",
   "metadata": {},
   "source": [
    "* So here in the above image, 'authority', 'event', 'has tail', 'rich' and 'gender' are called features. The vectors generatd for King, Man, Woman, Queen, Battle, and Horse are called feature vectors.\n",
    "* Generatig features vectors manually is very very difficult, so for generating these feature vectors we use neural networks, and neural network will authomatically learn these features.\n",
    "* Generating these feature vectors are also called word embedding.\n",
    "* The wonderful thing about word embedding is, that we don't know in vector 'King' the first 1 is 'authority', it works magically.\n",
    "* **See the fake problem of missing words in a sentence as whown in the following images:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2bc70",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffdc05",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34ca94",
   "metadata": {},
   "source": [
    "* So now as shown in the following images, if I take a window of three words, and I say if there is a words 'lived' and 'a', so then I can preict that there is a word 'there'. So here we're taking the 2nd and 3rd words and we're predicting the 3rd word.\n",
    "* So these are our training samples. (We move the window of three words over the paragraph & generate all train samples. As result the generated training samples becomes the training set for the neural network.):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522204ab",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f2595",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1bb116",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a0fa1",
   "metadata": {},
   "source": [
    "* In the training samples, the words in left side are X and on the right side is Y. You feed the words X and predict the word Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da468a60",
   "metadata": {},
   "source": [
    "* Now to train our neural network using each of these samples, first let's we have words 'ordered' and 'his', based on these two words we want to predict the word 'king'. So the input layer will have One Hot Encoded vectors. If we have 5000 words in the vocabulary, then in input layer there will be a vector which has 5000 size and only one word will be one and the remaining will be zero. For exaple if the word is 'ordered' then in the vector the value of word 'ordered' will be 1 and the remaining numbers will be zero. Same for word 'his'. \n",
    "* Vocabulary means unique word in your text corpus.\n",
    "* In the hidden layer we have 4 neurons, and these 4 neuron are the size of the embedding vectors. The size may be different, there is no any rule, just trail and error.\n",
    "* In the output layer we'll have a 5000 size vector. So when we feed these training samples into neural network, what happen is, first the edges will have random values (weights), based on these random weights the output will be generated which might be wrong most likely. So we compare the actual output Y with the predicted output Y'. We take a loss which is a difference between the actual output and the predicted output and we back propagate. When we back propagate, essentially what we do is audjusting the weights, then we take a second sample, third sample ... we take all 5000 or 100000 samples & we train the model in such a way that if our input is 'ordered' and 'his' then the output should be 'King'.\n",
    "* **The process is shown in images bellow:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06876022",
   "metadata": {},
   "source": [
    "<img src = \"img6.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19948c58",
   "metadata": {},
   "source": [
    "<img src = \"img7.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5d17c",
   "metadata": {},
   "source": [
    "* **The second sample is (which is window of size 3):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1ba57",
   "metadata": {},
   "source": [
    "<img src = \"img8.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff2819",
   "metadata": {},
   "source": [
    "* **The 3rd sample:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36555d8",
   "metadata": {},
   "source": [
    "<img src = \"img9.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9128fa",
   "metadata": {},
   "source": [
    "* So when we feed let's say 1 milion samples and also we run 100 epochs, so our neural network will be trained. At that point the word vector for 'King' would be the weights which is shown in the following image: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d17b01",
   "metadata": {},
   "source": [
    "<img src = \"img10.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a7659",
   "metadata": {},
   "source": [
    "* Those weights are nothing but a trained word vector. And these vectors will be very similar with the vectors of word 'emperor' because the input is same. See the bellow image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef72cc4",
   "metadata": {},
   "source": [
    "<img src = \"img11.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15181ba0",
   "metadata": {},
   "source": [
    "* So the upper approach is called **Continues Bag Of Words (CBOW).**\n",
    "* In CBOW based on context we predict the target word.\n",
    "* There is 2nd methodology which is called **Skip Gram**. in skip gram we do reverse, we have a target word and based on that we predict the context words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9ac1b",
   "metadata": {},
   "source": [
    "<img src = \"img12.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ba4e5",
   "metadata": {},
   "source": [
    "* To **summarize,** Word2Vec is not  single method, but it can use one of the two techniques which is **CBOW** and **Skip Gram** to learn word embedding.\n",
    "* Simply **Word2Vec** means converting word into vectors.\n",
    "* The **Skip Gram** working mechanism is shown in bellow images:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3f815",
   "metadata": {},
   "source": [
    "<img src = \"img13.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bba2b",
   "metadata": {},
   "source": [
    "<img src = \"img14.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5578e4",
   "metadata": {},
   "source": [
    "<img src = \"img15.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbe62d",
   "metadata": {},
   "source": [
    "* So when you're using **Skip Gram**, the word embedding (weights) is a layer between the input layer and a hidden layer, but when you're using **CBOW**, the word embedding (weights) is between hidden layer and the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fbd71",
   "metadata": {},
   "source": [
    "<img src = \"img16.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4968f8",
   "metadata": {},
   "source": [
    "<img src = \"img17.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2109a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
