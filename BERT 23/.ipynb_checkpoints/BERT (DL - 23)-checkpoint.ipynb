{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115bae0a",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a pre-trained deep learning model developed by Google for natural language processing (NLP) tasks. It uses a transformer-based architecture to learn contextual relations between words in a sentence or text. BERT is trained on a large corpus of text data and can be fine-tuned for various NLP tasks such as question answering, sentiment analysis, and text classification. BERT has achieved state-of-the-art results on several benchmark datasets in the NLP field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f060b",
   "metadata": {},
   "source": [
    "Let's assume you work on a text classification task where input to the model is a word and you want to classify the words either to be person or country. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc8252",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbaf65",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d86d6e",
   "metadata": {},
   "source": [
    "* Now think how the model process these words? Means how can we capture similarities between two words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e168fa",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df605345",
   "metadata": {},
   "source": [
    "* So the simple answer to the question is: We look to the features of each input word and then compare them with each other if they're similary then will be classify in the same category as shown in the bellow image:\n",
    "* The image shows us that the first two homes are similar but the third one is not similar with previous two homes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f1e77",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235dbfc",
   "metadata": {},
   "source": [
    "* Similarly we represent the person and country names as features and finally we can decide whether it's person or country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96f7b6",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e8822",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9594056",
   "metadata": {},
   "source": [
    "<img src = \"img6.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d2477",
   "metadata": {},
   "source": [
    "* So the approach covered in the upper images are representing text as number using 'Word2Vec'. The main issue of 'Word2Vec' is, it's generate fixed embedding or generate fixed vectors for similar words. It means one word using with different context might give different meaning, so this is the main issue using 'Word2Vec'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df73e0",
   "metadata": {},
   "source": [
    "<img src = \"img7.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6ed8c",
   "metadata": {},
   "source": [
    "* So based on the mentioned problem, we need to have a model to generate contextualize meaning of a word. It means based on studying the whole sentence model will generate a number representation for a specifict word. \n",
    "### So BERT allow us to do the exact same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d9b87",
   "metadata": {},
   "source": [
    "* BERT can generate contextualized embedding. At the same time it will capture the meaning of the word in a right way. As we see in the following image the words 'fair' and 'unbiased' are a kind of similar, so the model will generate almost the same vectors. Similarly the word 'fair' in sentence number 3 in the following image and the word 'Canival' has most similarity, so the model will generate almost the same vectors for these two words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0cf85",
   "metadata": {},
   "source": [
    "<img src = \"img8.png\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0de3d2",
   "metadata": {},
   "source": [
    "* **BERT** can also generate the embedding for the entire sentence. For the whole sentence it will generate a single vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febe255",
   "metadata": {},
   "source": [
    "<img src = \"img9.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08df66d",
   "metadata": {},
   "source": [
    "* BERT was trained by Google on **2500 Milions words of Wikipedia** and **800 Milions words of different books.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe27d0",
   "metadata": {},
   "source": [
    "<img src = \"img10.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897b50d",
   "metadata": {},
   "source": [
    "* For training they use two approaches:\n",
    "    1. Mased Language Model\n",
    "    2. Next sentence prediction\n",
    "* So today **Google Search** is powered by BERT. When you searching something, when you type the first 1 word or 2 words then they will give you suggestions for your search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc84dd",
   "metadata": {},
   "source": [
    "<img src = \"img11.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a5ace",
   "metadata": {},
   "source": [
    "* Links for more about BERT:\n",
    "    http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c978cf",
   "metadata": {},
   "source": [
    "* Now let's look into tensorflow code and generate some sentence and words embedding.\n",
    "* So to locate BERT model we go to **tensorflow hub** which is the repository of all the different models. Go to word embedding then to BERT, here you will see a section for BERT. BERT has different models which is listed here... so we'll use the simple one which has 12 encoders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58eb239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries:\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13809858",
   "metadata": {},
   "source": [
    "* BERT models tensorflow hub link: https://tfhub.dev/google/collections/bert/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best thing with BERT is that, we can directly take the URL and past it in the Jupyter ('encoders') to use it. \n",
    "# Then for each model we have a pre-processing URL which can pre-process you text.\n",
    "\n",
    "# preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "# encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "\n",
    "encoder_url = \"bert_en_uncased_L-12_H-768_A-12_4\"\n",
    "preprocess_url = \"bert_en_uncased_preprocess_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1122463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So next we create a hub layer which takes pre-process URL as an argument. It gives you like function pointer. Then we can \n",
    "# supply a buch of statements and it will do pre-process on those statements.\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(preprocess_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79dcbf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_mask', 'input_type_ids', 'input_word_ids'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So let's say here we build a movie classification model. So we pass couple of statements to be pre-processed by the model.\n",
    "# The output of this function pointer is gonna be dictionary so we just print the key becaus the object may be big:\n",
    "text_test = ['nice movie indeed','I love python programming']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c70ed92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the two sentences are pre-processed.\n",
    "# Now we can check individual elements in this dictionary. The first one is 'input_mask'. \n",
    "text_preprocessed['input_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2a890",
   "metadata": {},
   "source": [
    "* So we see the shape is (2, 128), 2 is because of: we have two sentences, and for each sentence we see the mask. So as we see the first sentence has three words but the BERT generated 5 ones (1s). The reason is the way BERT work, it add a special token called 'CLS' at the beginning and to separte two sentences it use 'SEP' token. Now if we count it it will be 5. The idea is similar for the second sentece. 128 is a kind of maximum lenght of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ef81f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[  101,  3835,  3185,  5262,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1045,  2293, 18750,  4730,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the next element is 'input_type_ids' which is pretty usefull. So it assign special number to each word because this is \n",
    "# pre-proecssing step, in next step we do word embedding.\n",
    "\n",
    "# text_preprocessed['input_type_ids']\n",
    "text_preprocessed['input_word_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f4af5",
   "metadata": {},
   "source": [
    "### Special Tokens:\n",
    "* **101 --> CLS token**\n",
    "* **102 --> SEP token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1934430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['default', 'encoder_outputs', 'pooled_output', 'sequence_output'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once the pre-processing steps is done, we want to create another layer and it will have the encoder URL.\n",
    "# This layer we'll called BERT model. So as before it will return like function pointer and we can supply our pre-process \n",
    "# text and as result it will generate word embedding for the sentencess.\n",
    "# Again we just call the keys.\n",
    "\n",
    "bert_model = hub.KerasLayer(encoder_url)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e419754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.7917739 , -0.214119  ,  0.49769554, ...,  0.2446523 ,\n",
       "        -0.4733446 ,  0.8175868 ],\n",
       "       [-0.91712296, -0.4793517 , -0.7865697 , ..., -0.6175173 ,\n",
       "        -0.7102685 ,  0.92184293]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So this will have three keys, the first one is 'pooled_output'. \n",
    "# 'Pooled_output' is the embedding for the entire sentence.\n",
    "bert_results['pooled_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d99ca",
   "metadata": {},
   "source": [
    "* So we see, we had two sentences, and we see the embedding for the two sentences. The embedding vector size is 768. So this vector nicely represent the statement 'nice movie indeed' in form of numbers. Similarly we have other embedding vector for the 2nd sentence.\n",
    "* So these embeddings are pretty powerful and we can use it for our NLP task, it could be movie review classification, name entity recognization, it could be anything, but BERT help you to generate a meaningful vectors out of your statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b78f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.07292053,  0.08567811,  0.14476846, ..., -0.09677067,\n",
       "          0.08722128,  0.07711104],\n",
       "        [ 0.1783935 , -0.19006078,  0.5034942 , ..., -0.05869839,\n",
       "          0.32717115, -0.15578555],\n",
       "        [ 0.18701448, -0.43388778, -0.48875168, ..., -0.15502736,\n",
       "          0.00145129, -0.2447096 ],\n",
       "        ...,\n",
       "        [ 0.12083042,  0.1288426 ,  0.4645356 , ...,  0.07375526,\n",
       "          0.17441978,  0.165221  ],\n",
       "        [ 0.07967852, -0.01190632,  0.50225437, ...,  0.13777742,\n",
       "          0.21002209,  0.00624598],\n",
       "        [-0.07212704, -0.28303427,  0.5903336 , ...,  0.47551903,\n",
       "          0.16668484, -0.08920316]],\n",
       "\n",
       "       [[-0.0790059 ,  0.3633513 , -0.21101557, ..., -0.1718373 ,\n",
       "          0.16299753,  0.6724265 ],\n",
       "        [ 0.27883515,  0.43716335, -0.3576473 , ..., -0.04463643,\n",
       "          0.38315186,  0.5887984 ],\n",
       "        [ 1.2037671 ,  1.0727018 ,  0.4840877 , ...,  0.24921034,\n",
       "          0.40730911,  0.4048181 ],\n",
       "        ...,\n",
       "        [ 0.08630013,  0.1935384 ,  0.47540036, ...,  0.18880169,\n",
       "         -0.06474102,  0.31318596],\n",
       "        [ 0.15887041,  0.2857266 ,  0.3734078 , ...,  0.09309134,\n",
       "         -0.04969549,  0.38761112],\n",
       "        [-0.08079888, -0.09572833,  0.26809767, ...,  0.13979661,\n",
       "         -0.06315826,  0.2728833 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's look out to the 2nd key which is 'sequence_output'. \n",
    "# 'sequence_output' is individual word embedding vectors. 2 is again for 2 sentences, 128 means: we've 128 padding for each\n",
    "# sentence and for each of the word inside the sentence we will have 768 size vector.\n",
    "# The paddings (numbers) are because of contextualize embedding. \n",
    "bert_results['sequence_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0bfd713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you look at 'encoder_output', if we display the len of 'encoder_output', it will be 12. The reason is we're use the \n",
    "# small size BERT base. Means this BERT model has 12 encoder layers and each layer has 768 size embedding vectors.\n",
    "len(bert_results['encoder_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b6195d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.12901425,  0.00644747, -0.0361497 , ...,  0.04999633,\n",
       "          0.06149192, -0.02657545],\n",
       "        [ 1.1753384 ,  1.2140784 ,  1.1569979 , ...,  0.11634396,\n",
       "         -0.35855335, -0.40490183],\n",
       "        [ 0.03859042,  0.5386996 , -0.21089777, ...,  0.21858197,\n",
       "          0.7260167 , -1.1158603 ],\n",
       "        ...,\n",
       "        [-0.07587016, -0.254219  ,  0.7075511 , ...,  0.50541997,\n",
       "         -0.1887868 ,  0.15028326],\n",
       "        [-0.16066615, -0.28089687,  0.5759706 , ...,  0.52758557,\n",
       "         -0.11141385,  0.02887541],\n",
       "        [-0.0442816 , -0.2027959 ,  0.5909355 , ...,  0.8133834 ,\n",
       "         -0.39075816, -0.02601739]],\n",
       "\n",
       "       [[ 0.1890359 ,  0.02752548, -0.0651374 , ..., -0.00620213,\n",
       "          0.15053894,  0.03165445],\n",
       "        [ 0.5916149 ,  0.7589137 , -0.07240661, ...,  0.6190394 ,\n",
       "          0.8292891 ,  0.16161954],\n",
       "        [ 1.4460827 ,  0.44602644,  0.4099025 , ...,  0.48255914,\n",
       "          0.62691146,  0.13463417],\n",
       "        ...,\n",
       "        [ 0.15147898, -0.21573842,  0.70329076, ..., -0.12537211,\n",
       "         -0.13787258,  0.2772205 ],\n",
       "        [ 0.051438  , -0.24052706,  0.5356913 , ..., -0.07915058,\n",
       "         -0.03307928,  0.1738092 ],\n",
       "        [ 0.20934707, -0.15645266,  0.60395443, ...,  0.3290352 ,\n",
       "         -0.35827187,  0.08100392]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output of 'encoder_output' is nothing but the output of each individual encoder. \n",
    "bert_results['encoder_outputs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cef44e",
   "metadata": {},
   "source": [
    "* To get more about the elements, you can simply check the https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4 link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335cc9d5",
   "metadata": {},
   "source": [
    "* **So here next we do email classification using BERT whether email is spam or not spam?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b2760",
   "metadata": {},
   "source": [
    "<img src = \"img12.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebce6e4",
   "metadata": {},
   "source": [
    "* So BERT will convert the email sentences (the whole email) into an embedding vectors as we saw before that the whole concept of BERT is to generate the embedding vectors for the entire sentence, so that is something that we can feed it to the neural network and do the training.\n",
    "* So we'll generate an embedding vectors of 768 lenght, and then we'll supply a very simple neural network with only one dense layer (one neuron in the dense layer). As an output wi'll also put a dropout layer just to tackle the model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30002e5f",
   "metadata": {},
   "source": [
    "<img src = \"img13.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289789eb",
   "metadata": {},
   "source": [
    "* Now when you open the BERT box by the way, it has two components (pre-process and encoding) which we covered them previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983ea59",
   "metadata": {},
   "source": [
    "<img src = \"img14.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ecf46",
   "metadata": {},
   "source": [
    "* Let's jump to the coding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b3a93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries:\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efc60c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the dataset which we have here is having two columns, the first one is email category and the second one is the email\n",
    "# text. So let's simply read the CSV file into the notebook:\n",
    "df = pd.read_csv(\"spam.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18a64f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>641</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Message                                                            \\\n",
       "           count unique                                                top   \n",
       "Category                                                                     \n",
       "ham         4825   4516                             Sorry, I'll call later   \n",
       "spam         747    641  Please call our customer service representativ...   \n",
       "\n",
       "               \n",
       "         freq  \n",
       "Category       \n",
       "ham        30  \n",
       "spam        4  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now let's do some basic analysis. \n",
    "df.groupby(\"Category\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52144457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we clearly see some imbalance in the dataset. \n",
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78489165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15481865284974095"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So here we use 'Under-sampling' method to tackle with this problem.\n",
    "# Let's check the ratio between thses two classes:\n",
    "747/4825"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c3f8b",
   "metadata": {},
   "source": [
    "* So we see that 15% are spam emails and 85% are non spam emails. So the under-sampling method says that from 4825 non spam emails just pick up 747 samples and discards the others and then join it with the spam (747) emails and train the model. This is not good approach but to keep thing simple we go through this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "694c2c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So here we create a new DataFrame for 'spam' emails:\n",
    "df_spam = df[df['Category']=='spam']\n",
    "df_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb5cf58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4825, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly we create a new DataFrame for 'ham' emails:\n",
    "df_ham = df[df['Category']=='ham']\n",
    "df_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd2fb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>ham</td>\n",
       "      <td>Storming msg: Wen u lift d phne, u say \"HELLO\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>ham</td>\n",
       "      <td>Total video converter free download type this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "1385      ham  Storming msg: Wen u lift d phne, u say \"HELLO\"...\n",
       "1506      ham  Total video converter free download type this ..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now if we do like 'df_ham.sample(2)', then it will return 2 random samples.\n",
    "df_ham.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e33e6840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So instead of 2 samples we need 747 samples.\n",
    "df_ham_downsampled = df_ham.sample(df_spam.shape[0])\n",
    "df_ham_downsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6a6e1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now we can join these two dataset for model training.\n",
    "df_balanced = pd.concat([df_ham_downsampled, df_spam])\n",
    "df_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38d877f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     747\n",
       "spam    747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to check the samples from both classes:\n",
    "df_balanced['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8342367b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>spam</td>\n",
       "      <td>WIN a £200 Shopping spree every WEEK Starting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>spam</td>\n",
       "      <td>3. You have received your mobile content. Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>spam</td>\n",
       "      <td>all the lastest from Stereophonics, Marley, Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>ham</td>\n",
       "      <td>Maybe?! Say hi to  and find out if  got his ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>ham</td>\n",
       "      <td>U wan 2 haf lunch i'm in da canteen now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "5073     spam  WIN a £200 Shopping spree every WEEK Starting ...\n",
       "4473     spam    3. You have received your mobile content. Enjoy\n",
       "907      spam  all the lastest from Stereophonics, Marley, Di...\n",
       "677       ham  Maybe?! Say hi to  and find out if  got his ca...\n",
       "4756      ham           U wan 2 haf lunch i'm in da canteen now."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now just to priint some samples from the balanced data frame:\n",
    "df_balanced.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b935abb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>spam</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>ham</td>\n",
       "      <td>If you're still up, maybe leave the credit car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>ham</td>\n",
       "      <td>Cool, I'll text you when I'm on the way</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>ham</td>\n",
       "      <td>Had the money issue weigh me down but thanks t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>ham</td>\n",
       "      <td>Lol now I'm after that hot air balloon!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message  spam\n",
       "4628     spam  Please call our customer service representativ...     1\n",
       "4751      ham  If you're still up, maybe leave the credit car...     0\n",
       "2459      ham            Cool, I'll text you when I'm on the way     0\n",
       "2361      ham  Had the money issue weigh me down but thanks t...     0\n",
       "1168      ham            Lol now I'm after that hot air balloon!     0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we create a new column for spam and ham to be represented as 1 and 0.\n",
    "df_balanced['spam']=df_balanced['Category'].apply(lambda x: 1 if x=='spam' else 0)\n",
    "df_balanced.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b01cbadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the DataFrame is ready and we can call train_test_split method for generating train and test samples.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced['Message'],df_balanced['spam'], stratify=df_balanced['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a54c892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442                You were supposed to wake ME up &gt;:(\n",
       "4226    The world suffers a lot... Not because of the ...\n",
       "3244    Pls accept me for one day. Or am begging you c...\n",
       "899     Thursday night? Yeah, sure thing, we'll work i...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see some train values:\n",
    "X_train.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d24c8",
   "metadata": {},
   "source": [
    "**Now let's using BERT model get embeding vectors for few sample statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1f5d387",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     preprocessed_text \u001b[38;5;241m=\u001b[39m bert_preprocess(sentences)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bert_encoder(preprocessed_text)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m \u001b[43mget_sentence_embeding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m500$ discount. hurry up\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBhavin, are you up for a volleybal game tomorrow?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36mget_sentence_embeding\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentence_embeding\u001b[39m(sentences):\n\u001b[1;32m----> 7\u001b[0m     preprocessed_text \u001b[38;5;241m=\u001b[39m \u001b[43mbert_preprocess\u001b[49m(sentences)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bert_encoder(preprocessed_text)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# So now as we have seen previously, we supply sentence to BERT model to generate the length vectors. So we want to create a\n",
    "# simple function to take sentence as input and return the embedded vectors as output.\n",
    "# So first the function takes the sentences, first we apply pre-processing and as result we get pre-precessed text and then \n",
    "# that we suppply to encoder. Then it will return a dictionary and from dictionary it will return 'pooled_output'.\n",
    "\n",
    "def get_sentence_embeding(sentences):\n",
    "    preprocessed_text = bert_preprocess(sentences)\n",
    "    return bert_encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "get_sentence_embeding([\n",
    "    \"500$ discount. hurry up\", \n",
    "    \"Bhavin, are you up for a volleybal game tomorrow?\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
