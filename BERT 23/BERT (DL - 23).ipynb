{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeeefde1",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a pre-trained deep learning model developed by Google for natural language processing (NLP) tasks. It uses a transformer-based architecture to learn contextual relations between words in a sentence or text. BERT is trained on a large corpus of text data and can be fine-tuned for various NLP tasks such as question answering, sentiment analysis, and text classification. BERT has achieved state-of-the-art results on several benchmark datasets in the NLP field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567bf7d5",
   "metadata": {},
   "source": [
    "Let's assume you work on a text classification task where input to the model is a word and you want to classify the words either to be person or country. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e5732",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e2efa",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b883b",
   "metadata": {},
   "source": [
    "* Now think how the model process these words? Means how can we capture similarities between two words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b824c1",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8959668",
   "metadata": {},
   "source": [
    "* So the simple answer to the question is: We look to the features of each input word and then compare them with each other if they're similary then will be classify in the same category as shown in the bellow image:\n",
    "* The image shows us that the first two homes are similar but the third one is not similar with previous two homes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c8b5c",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98244bd",
   "metadata": {},
   "source": [
    "* Similarly we represent the person and country names as features and finally we can decide whether it's person or country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d4728",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979df6e3",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf375f8",
   "metadata": {},
   "source": [
    "<img src = \"img6.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7a87f",
   "metadata": {},
   "source": [
    "* So the approach covered in the upper images are representing text as number using 'Word2Vec'. The main issue of 'Word2Vec' is, it's generate fixed embedding or generate fixed vectors for similar words. It means one word using with different context might give different meaning, so this is the main issue using 'Word2Vec'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef142094",
   "metadata": {},
   "source": [
    "<img src = \"img7.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec6f91",
   "metadata": {},
   "source": [
    "* So based on the mentioned problem, we need to have a model to generate contextualize meaning of a word. It means based on studying the whole sentence model will generate a number representation for a specifict word. \n",
    "### So BERT allow us to do the exact same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d8850",
   "metadata": {},
   "source": [
    "* BERT can generate contextualized embedding. At the same time it will capture the meaning of the word in a right way. As we see in the following image the words 'fair' and 'unbiased' are a kind of similar, so the model will generate almost the same vectors. Similarly the word 'fair' in sentence number 3 in the following image and the word 'Canival' has most similarity, so the model will generate almost the same vectors for these two words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95fa1d",
   "metadata": {},
   "source": [
    "<img src = \"img8.png\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66519bde",
   "metadata": {},
   "source": [
    "* **BERT** can also generate the embedding for the entire sentence. For the whole sentence it will generate a single vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e154b8",
   "metadata": {},
   "source": [
    "<img src = \"img9.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa6834",
   "metadata": {},
   "source": [
    "* BERT was trained by Google on **2500 Milions words of Wikipedia** and **800 Milions words of different books.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f84f2",
   "metadata": {},
   "source": [
    "<img src = \"img10.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c375059",
   "metadata": {},
   "source": [
    "* For training they use two approaches:\n",
    "    1. Mased Language Model\n",
    "    2. Next sentence prediction\n",
    "* So today **Google Search** is powered by BERT. When you searching something, when you type the first 1 word or 2 words then they will give you suggestions for your search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e836f",
   "metadata": {},
   "source": [
    "<img src = \"img11.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2aae16",
   "metadata": {},
   "source": [
    "* Links for more about BERT:\n",
    "    http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3986c",
   "metadata": {},
   "source": [
    "* Now let's look into tensorflow code and generate some sentence and words embedding.\n",
    "* So to locate BERT model we go to **tensorflow hub** which is the repository of all the different models. Go to word embedding then to BERT, here you will see a section for BERT. BERT has different models which is listed here... so we'll use the simple one which has 12 encoders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91cccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries:\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e5829",
   "metadata": {},
   "source": [
    "* BERT models tensorflow hub link: https://tfhub.dev/google/collections/bert/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb125fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best thing with BERT is that, we can directly take the URL and past it in the Jupyter ('encoders') to use it. \n",
    "# Then for each model we have a pre-processing URL which can pre-process you text.\n",
    "preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7666580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So next we create a hub layer which takes pre-process URL as an argument. It gives you like function pointer. Then we can \n",
    "# supply a buch of statements and it will do pre-process on those statements.\n",
    "\n",
    "# bert_preprocess_model = hub.KerasLayer(preprocess_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's say here we build a movie classification model. So we pass couple of statements to be pre-processed by the model.\n",
    "# The output of this function pointer is gonna be dictionary so we just print the key becaus the object may be big:\n",
    "text_test = ['nice movie indeed','I love python programming']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the two sentences are pre-processed.\n",
    "# Now we can check individual elements in this dictionary. The first one is 'input_mask'. \n",
    "text_preprocessed['input_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0392cb2",
   "metadata": {},
   "source": [
    "* So we see the shape is (2, 128), 2 is because of: we have two sentences, and for each sentence we see the mask. So as we see the first sentence has three words but the BERT generated 5 ones (1s). The reason is the way BERT work, it add a special token called 'CLS' at the beginning and to separte two sentences it use 'SEP' token. Now if we count it it will be 5. The idea is similar for the second sentece. 128 is a kind of maximum lenght of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e81103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the next element is 'input_type_ids' which is pretty usefull. So it assign special number to each word because this is \n",
    "# pre-proecssing step, in next step we do word embedding.\n",
    "\n",
    "# text_preprocessed['input_type_ids']\n",
    "text_preprocessed['input_word_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa35846",
   "metadata": {},
   "source": [
    "### Special Tokens:\n",
    "* **101 --> CLS token**\n",
    "* **102 --> SEP token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b91404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the pre-processing steps is done, we want to create another layer and it will have the encoder URL.\n",
    "# This layer we'll called BERT model. So as before it will return like function pointer and we can supply our pre-process \n",
    "# text and as result it will generate word embedding for the sentencess.\n",
    "# Again we just call the keys.\n",
    "\n",
    "bert_model = hub.KerasLayer(encoder_url)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49362fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this will have three keys, the first one is 'pooled_output'. \n",
    "# 'Pooled_output' is the embedding for the entire sentence.\n",
    "bert_results['pooled_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42fc937",
   "metadata": {},
   "source": [
    "* So we see, we had two sentences, and we see the embedding for the two sentences. The embedding vector size is 768. So this vector nicely represent the statement 'nice movie indeed' in form of numbers. Similarly we have other embedding vector for the 2nd sentence.\n",
    "* So these embeddings are pretty powerful and we can use it for our NLP task, it could be movie review classification, name entity recognization, it could be anything, but BERT help you to generate a meaningful vectors out of your statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look out to the 2nd key which is 'sequence_output'. \n",
    "# 'sequence_output' is individual word embedding vectors. 2 is again for 2 sentences, 128 means: we've 128 padding for each\n",
    "# sentence and for each of the word inside the sentence we will have 768 size vector.\n",
    "# The paddings (numbers) are because of contextualize embedding. \n",
    "bert_results['sequence_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ae499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you look at 'encoder_output', if we display the len of 'encoder_output', it will be 12. The reason is we're use the \n",
    "# small size BERT base. Means this BERT model has 12 encoder layers and each layer has 768 size embedding vectors.\n",
    " len(bert_results['encoder_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c90a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of 'encoder_output' is nothing but the output of each individual encoder. \n",
    "bert_results['encoder_outputs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cece02",
   "metadata": {},
   "source": [
    "* To get more about the elements, you can simply check the https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4 link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6989fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c3f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b109c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974670a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src = \"img12.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
