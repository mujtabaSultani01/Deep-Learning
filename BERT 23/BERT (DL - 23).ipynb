{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115bae0a",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a pre-trained deep learning model developed by Google for natural language processing (NLP) tasks. It uses a transformer-based architecture to learn contextual relations between words in a sentence or text. BERT is trained on a large corpus of text data and can be fine-tuned for various NLP tasks such as question answering, sentiment analysis, and text classification. BERT has achieved state-of-the-art results on several benchmark datasets in the NLP field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f060b",
   "metadata": {},
   "source": [
    "Let's assume you work on a text classification task where input to the model is a word and you want to classify the words either to be person or country. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc8252",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbaf65",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d86d6e",
   "metadata": {},
   "source": [
    "* Now think how the model process these words? Means how can we capture similarities between two words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e168fa",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df605345",
   "metadata": {},
   "source": [
    "* So the simple answer to the question is: We look to the features of each input word and then compare them with each other if they're similary then will be classify in the same category as shown in the bellow image:\n",
    "* The image shows us that the first two homes are similar but the third one is not similar with previous two homes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f1e77",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235dbfc",
   "metadata": {},
   "source": [
    "* Similarly we represent the person and country names as features and finally we can decide whether it's person or country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96f7b6",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e8822",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9594056",
   "metadata": {},
   "source": [
    "<img src = \"img6.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d2477",
   "metadata": {},
   "source": [
    "* So the approach covered in the upper images are representing text as number using 'Word2Vec'. The main issue of 'Word2Vec' is, it's generate fixed embedding or generate fixed vectors for similar words. It means one word using with different context might give different meaning, so this is the main issue using 'Word2Vec'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df73e0",
   "metadata": {},
   "source": [
    "<img src = \"img7.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6ed8c",
   "metadata": {},
   "source": [
    "* So based on the mentioned problem, we need to have a model to generate contextualize meaning of a word. It means based on studying the whole sentence model will generate a number representation for a specifict word. \n",
    "### So BERT allow us to do the exact same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d9b87",
   "metadata": {},
   "source": [
    "* BERT can generate contextualized embedding. At the same time it will capture the meaning of the word in a right way. As we see in the following image the words 'fair' and 'unbiased' are a kind of similar, so the model will generate almost the same vectors. Similarly the word 'fair' in sentence number 3 in the following image and the word 'Canival' has most similarity, so the model will generate almost the same vectors for these two words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0cf85",
   "metadata": {},
   "source": [
    "<img src = \"img8.png\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0de3d2",
   "metadata": {},
   "source": [
    "* **BERT** can also generate the embedding for the entire sentence. For the whole sentence it will generate a single vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febe255",
   "metadata": {},
   "source": [
    "<img src = \"img9.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08df66d",
   "metadata": {},
   "source": [
    "* BERT was trained by Google on **2500 Milions words of Wikipedia** and **800 Milions words of different books.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe27d0",
   "metadata": {},
   "source": [
    "<img src = \"img10.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897b50d",
   "metadata": {},
   "source": [
    "* For training they use two approaches:\n",
    "    1. Mased Language Model\n",
    "    2. Next sentence prediction\n",
    "* So today **Google Search** is powered by BERT. When you searching something, when you type the first 1 word or 2 words then they will give you suggestions for your search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc84dd",
   "metadata": {},
   "source": [
    "<img src = \"img11.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a5ace",
   "metadata": {},
   "source": [
    "* Links for more about BERT:\n",
    "    http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c978cf",
   "metadata": {},
   "source": [
    "* Now let's look into tensorflow code and generate some sentence and words embedding.\n",
    "* So to locate BERT model we go to **tensorflow hub** which is the repository of all the different models. Go to word embedding then to BERT, here you will see a section for BERT. BERT has different models which is listed here... so we'll use the simple one which has 12 encoders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58eb239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries:\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13809858",
   "metadata": {},
   "source": [
    "* BERT models tensorflow hub link: https://tfhub.dev/google/collections/bert/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best thing with BERT is that, we can directly take the URL and past it in the Jupyter ('encoders') to use it. \n",
    "# Then for each model we have a pre-processing URL which can pre-process you text.\n",
    "\n",
    "# preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "# encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "\n",
    "encoder_url = \"bert_en_uncased_L-12_H-768_A-12_4\"\n",
    "preprocess_url = \"bert_en_uncased_preprocess_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1122463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# So next we create a hub layer which takes pre-process URL as an argument. It gives you like function pointer. Then we can \n",
    "# supply a buch of statements and it will do pre-process on those statements.\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(preprocess_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79dcbf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_mask', 'input_type_ids', 'input_word_ids'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So let's say here we build a movie classification model. So we pass couple of statements to be pre-processed by the model.\n",
    "# The output of this function pointer is gonna be dictionary so we just print the key becaus the object may be big:\n",
    "text_test = ['nice movie indeed','I love python programming']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c70ed92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the two sentences are pre-processed.\n",
    "# Now we can check individual elements in this dictionary. The first one is 'input_mask'. \n",
    "text_preprocessed['input_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2a890",
   "metadata": {},
   "source": [
    "* So we see the shape is (2, 128), 2 is because of: we have two sentences, and for each sentence we see the mask. So as we see the first sentence has three words but the BERT generated 5 ones (1s). The reason is the way BERT work, it add a special token called 'CLS' at the beginning and to separte two sentences it use 'SEP' token. Now if we count it it will be 5. The idea is similar for the second sentece. 128 is a kind of maximum lenght of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ef81f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[  101,  3835,  3185,  5262,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1045,  2293, 18750,  4730,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the next element is 'input_type_ids' which is pretty usefull. So it assign special number to each word because this is \n",
    "# pre-proecssing step, in next step we do word embedding.\n",
    "\n",
    "# text_preprocessed['input_type_ids']\n",
    "text_preprocessed['input_word_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f4af5",
   "metadata": {},
   "source": [
    "### Special Tokens:\n",
    "* **101 --> CLS token**\n",
    "* **102 --> SEP token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1934430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['default', 'encoder_outputs', 'pooled_output', 'sequence_output'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once the pre-processing steps is done, we want to create another layer and it will have the encoder URL.\n",
    "# This layer we'll called BERT model. So as before it will return like function pointer and we can supply our pre-process \n",
    "# text and as result it will generate word embedding for the sentencess.\n",
    "# Again we just call the keys.\n",
    "\n",
    "bert_model = hub.KerasLayer(encoder_url)\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e419754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.7917739 , -0.214119  ,  0.49769554, ...,  0.2446523 ,\n",
       "        -0.4733446 ,  0.8175868 ],\n",
       "       [-0.91712296, -0.4793517 , -0.7865697 , ..., -0.6175173 ,\n",
       "        -0.7102685 ,  0.92184293]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So this will have three keys, the first one is 'pooled_output'. \n",
    "# 'Pooled_output' is the embedding for the entire sentence.\n",
    "bert_results['pooled_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d99ca",
   "metadata": {},
   "source": [
    "* So we see, we had two sentences, and we see the embedding for the two sentences. The embedding vector size is 768. So this vector nicely represent the statement 'nice movie indeed' in form of numbers. Similarly we have other embedding vector for the 2nd sentence.\n",
    "* So these embeddings are pretty powerful and we can use it for our NLP task, it could be movie review classification, name entity recognization, it could be anything, but BERT help you to generate a meaningful vectors out of your statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b78f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.07292053,  0.08567811,  0.14476846, ..., -0.09677067,\n",
       "          0.08722128,  0.07711104],\n",
       "        [ 0.1783935 , -0.19006078,  0.5034942 , ..., -0.05869839,\n",
       "          0.32717115, -0.15578555],\n",
       "        [ 0.18701448, -0.43388778, -0.48875168, ..., -0.15502736,\n",
       "          0.00145129, -0.2447096 ],\n",
       "        ...,\n",
       "        [ 0.12083042,  0.1288426 ,  0.4645356 , ...,  0.07375526,\n",
       "          0.17441978,  0.165221  ],\n",
       "        [ 0.07967852, -0.01190632,  0.50225437, ...,  0.13777742,\n",
       "          0.21002209,  0.00624598],\n",
       "        [-0.07212704, -0.28303427,  0.5903336 , ...,  0.47551903,\n",
       "          0.16668484, -0.08920316]],\n",
       "\n",
       "       [[-0.0790059 ,  0.3633513 , -0.21101557, ..., -0.1718373 ,\n",
       "          0.16299753,  0.6724265 ],\n",
       "        [ 0.27883515,  0.43716335, -0.3576473 , ..., -0.04463643,\n",
       "          0.38315186,  0.5887984 ],\n",
       "        [ 1.2037671 ,  1.0727018 ,  0.4840877 , ...,  0.24921034,\n",
       "          0.40730911,  0.4048181 ],\n",
       "        ...,\n",
       "        [ 0.08630013,  0.1935384 ,  0.47540036, ...,  0.18880169,\n",
       "         -0.06474102,  0.31318596],\n",
       "        [ 0.15887041,  0.2857266 ,  0.3734078 , ...,  0.09309134,\n",
       "         -0.04969549,  0.38761112],\n",
       "        [-0.08079888, -0.09572833,  0.26809767, ...,  0.13979661,\n",
       "         -0.06315826,  0.2728833 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's look out to the 2nd key which is 'sequence_output'. \n",
    "# 'sequence_output' is individual word embedding vectors. 2 is again for 2 sentences, 128 means: we've 128 padding for each\n",
    "# sentence and for each of the word inside the sentence we will have 768 size vector.\n",
    "# The paddings (numbers) are because of contextualize embedding. \n",
    "bert_results['sequence_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0bfd713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you look at 'encoder_output', if we display the len of 'encoder_output', it will be 12. The reason is we're use the \n",
    "# small size BERT base. Means this BERT model has 12 encoder layers and each layer has 768 size embedding vectors.\n",
    "len(bert_results['encoder_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6195d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.12901425,  0.00644747, -0.0361497 , ...,  0.04999633,\n",
       "          0.06149192, -0.02657545],\n",
       "        [ 1.1753384 ,  1.2140784 ,  1.1569979 , ...,  0.11634396,\n",
       "         -0.35855335, -0.40490183],\n",
       "        [ 0.03859042,  0.5386996 , -0.21089777, ...,  0.21858197,\n",
       "          0.7260167 , -1.1158603 ],\n",
       "        ...,\n",
       "        [-0.07587016, -0.254219  ,  0.7075511 , ...,  0.50541997,\n",
       "         -0.1887868 ,  0.15028326],\n",
       "        [-0.16066615, -0.28089687,  0.5759706 , ...,  0.52758557,\n",
       "         -0.11141385,  0.02887541],\n",
       "        [-0.0442816 , -0.2027959 ,  0.5909355 , ...,  0.8133834 ,\n",
       "         -0.39075816, -0.02601739]],\n",
       "\n",
       "       [[ 0.1890359 ,  0.02752548, -0.0651374 , ..., -0.00620213,\n",
       "          0.15053894,  0.03165445],\n",
       "        [ 0.5916149 ,  0.7589137 , -0.07240661, ...,  0.6190394 ,\n",
       "          0.8292891 ,  0.16161954],\n",
       "        [ 1.4460827 ,  0.44602644,  0.4099025 , ...,  0.48255914,\n",
       "          0.62691146,  0.13463417],\n",
       "        ...,\n",
       "        [ 0.15147898, -0.21573842,  0.70329076, ..., -0.12537211,\n",
       "         -0.13787258,  0.2772205 ],\n",
       "        [ 0.051438  , -0.24052706,  0.5356913 , ..., -0.07915058,\n",
       "         -0.03307928,  0.1738092 ],\n",
       "        [ 0.20934707, -0.15645266,  0.60395443, ...,  0.3290352 ,\n",
       "         -0.35827187,  0.08100392]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output of 'encoder_output' is nothing but the output of each individual encoder. \n",
    "bert_results['encoder_outputs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cef44e",
   "metadata": {},
   "source": [
    "* To get more about the elements, you can simply check the https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4 link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335cc9d5",
   "metadata": {},
   "source": [
    "* **So here next we do email classification using BERT whether email is spam or not spam?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b2760",
   "metadata": {},
   "source": [
    "<img src = \"img12.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebce6e4",
   "metadata": {},
   "source": [
    "* So BERT will convert the email sentences (the whole email) into an embedding vectors as we saw before that the whole concept of BERT is to generate the embedding vectors for the entire sentence, so that is something that we can feed it to the neural network and do the training.\n",
    "* So we'll generate an embedding vectors of 768 lenght, and then we'll supply a very simple neural network with only one dense layer (one neuron in the dense layer). As an output wi'll also put a dropout layer just to tackle the model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30002e5f",
   "metadata": {},
   "source": [
    "<img src = \"img13.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289789eb",
   "metadata": {},
   "source": [
    "* Now when you open the BERT box by the way, it has two components (pre-process and encoding) which we covered them previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983ea59",
   "metadata": {},
   "source": [
    "<img src = \"img14.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ecf46",
   "metadata": {},
   "source": [
    "* Let's jump to the coding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3a93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries:\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc60c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the dataset which we have here is having two columns, the first one is email category and the second one is the email\n",
    "# text. So let's simply read the CSV file into the notebook:\n",
    "df = pd.read_csv(\"spam.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a64f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>641</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Message                                                            \\\n",
       "           count unique                                                top   \n",
       "Category                                                                     \n",
       "ham         4825   4516                             Sorry, I'll call later   \n",
       "spam         747    641  Please call our customer service representativ...   \n",
       "\n",
       "               \n",
       "         freq  \n",
       "Category       \n",
       "ham        30  \n",
       "spam        4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now let's do some basic analysis. \n",
    "df.groupby(\"Category\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52144457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we clearly see some imbalance in the dataset. \n",
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78489165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15481865284974095"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So here we use 'Under-sampling' method to tackle with this problem.\n",
    "# Let's check the ratio between thses two classes:\n",
    "747/4825"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c3f8b",
   "metadata": {},
   "source": [
    "* So we see that 15% are spam emails and 85% are non spam emails. So the under-sampling method says that from 4825 non spam emails just pick up 747 samples and discards the others and then join it with the spam (747) emails and train the model. This is not good approach but to keep thing simple we go through this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "694c2c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So here we create a new DataFrame for 'spam' emails:\n",
    "df_spam = df[df['Category']=='spam']\n",
    "df_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb5cf58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4825, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly we create a new DataFrame for 'ham' emails:\n",
    "df_ham = df[df['Category']=='ham']\n",
    "df_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2fb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>ham</td>\n",
       "      <td>Your dad is back in ph?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5049</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah so basically any time next week you can g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "3962      ham                            Your dad is back in ph?\n",
       "5049      ham  Yeah so basically any time next week you can g..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now if we do like 'df_ham.sample(2)', then it will return 2 random samples.\n",
    "df_ham.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e33e6840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So instead of 2 samples we need 747 samples.\n",
    "df_ham_downsampled = df_ham.sample(df_spam.shape[0])\n",
    "df_ham_downsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a6e1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now we can join these two dataset for model training.\n",
    "df_balanced = pd.concat([df_ham_downsampled, df_spam])\n",
    "df_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38d877f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     747\n",
       "spam    747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to check the samples from both classes:\n",
    "df_balanced['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8342367b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah, probably here for a while</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! We are trying to contact you. Last wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wat time do u wan 2 meet me later?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>spam</td>\n",
       "      <td>Great NEW Offer - DOUBLE Mins &amp; DOUBLE Txt on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>spam</td>\n",
       "      <td>FREE RING TONE just text \"POLYS\" to 87131. The...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "5283      ham                    Yeah, probably here for a while\n",
       "3807     spam  URGENT! We are trying to contact you. Last wee...\n",
       "3020      ham                 Wat time do u wan 2 meet me later?\n",
       "2708     spam  Great NEW Offer - DOUBLE Mins & DOUBLE Txt on ...\n",
       "2626     spam  FREE RING TONE just text \"POLYS\" to 87131. The..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now just to priint some samples from the balanced data frame:\n",
    "df_balanced.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b935abb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>spam</td>\n",
       "      <td>Dear Voucher Holder, 2 claim this weeks offer,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>spam</td>\n",
       "      <td>we tried to contact you re your response to ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>ham</td>\n",
       "      <td>HOW ARE U? I HAVE MISSED U! I HAVENT BEEN UP 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>spam</td>\n",
       "      <td>You are being contacted by our dating service ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>ham</td>\n",
       "      <td>U free on sat rite? U wan 2 watch infernal aff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message  spam\n",
       "635      spam  Dear Voucher Holder, 2 claim this weeks offer,...     1\n",
       "2670     spam  we tried to contact you re your response to ou...     1\n",
       "4529      ham  HOW ARE U? I HAVE MISSED U! I HAVENT BEEN UP 2...     0\n",
       "3334     spam  You are being contacted by our dating service ...     1\n",
       "4221      ham  U free on sat rite? U wan 2 watch infernal aff...     0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we create a new column for spam and ham to be represented as 1 and 0.\n",
    "df_balanced['spam']=df_balanced['Category'].apply(lambda x: 1 if x=='spam' else 0)\n",
    "df_balanced.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01cbadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the DataFrame is ready and we can call train_test_split method for generating train and test samples.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced['Message'],df_balanced['spam'], stratify=df_balanced['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a54c892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084    you are sweet as well, princess. Please tell m...\n",
       "1745    Someone has conacted our dating service and en...\n",
       "4681    That's cool he'll be here all night, lemme kno...\n",
       "967     I am not sure about night menu. . . I know onl...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see some train values:\n",
    "X_train.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d24c8",
   "metadata": {},
   "source": [
    "**Now let's get embedding vectors using BERT model for few sample statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1f5d387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.84351707, -0.5132726 , -0.8884572 , ..., -0.74748856,\n",
       "        -0.75314724,  0.91964495],\n",
       "       [-0.8720835 , -0.50543964, -0.94446677, ..., -0.8584752 ,\n",
       "        -0.7174535 ,  0.8808299 ]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now as we have seen previously, we call BERT model and supply sentences to BERT model to generate the length vectors. \n",
    "# So we want to create a simple function to take sentence as input and return the embedded vectors as output.\n",
    "# So first the function takes the sentences, first we apply pre-processing and as result we get pre-precessed text and then \n",
    "# that we suppply to encoder. Then it will return a dictionary and from dictionary it will return 'pooled_output'.\n",
    "\n",
    "bert_preprocess = hub.KerasLayer(preprocess_url)\n",
    "bert_encoder = hub.KerasLayer(encoder_url)\n",
    "\n",
    "def get_sentence_embeding(sentences):\n",
    "    preprocessed_text = bert_preprocess(sentences)\n",
    "    return bert_encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "get_sentence_embeding([\n",
    "    \"500$ discount. hurry up\", \n",
    "    \"Bhavin, are you up for a volleybal game tomorrow?\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad752a",
   "metadata": {},
   "source": [
    "* So we see it's returning a tensor of 2 with 768 shape. Each tensor is embedding for a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89c0d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's call 'get_sentence_embedding' for some sample words to see what is the benefits of having BERT encoding:\n",
    "e = get_sentence_embeding([\n",
    "    \"banana\", \n",
    "    \"grapes\",\n",
    "    \"mango\",\n",
    "    \"jeff bezos\",\n",
    "    \"elon musk\",\n",
    "    \"bill gates\"\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e48a5614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 768), dtype=float32, numpy=\n",
       "array([[-0.7606918 , -0.14219396,  0.49604586, ...,  0.42165315,\n",
       "        -0.5322141 ,  0.80312175],\n",
       "       [-0.86023223, -0.21242936,  0.4915693 , ...,  0.3979806 ,\n",
       "        -0.60506284,  0.8447163 ],\n",
       "       [-0.71288604, -0.15463905,  0.38401675, ...,  0.3527874 ,\n",
       "        -0.5099133 ,  0.73474073],\n",
       "       [-0.82533467, -0.35550582, -0.590697  , ..., -0.01613692,\n",
       "        -0.6141757 ,  0.872303  ],\n",
       "       [-0.75041336, -0.2681263 , -0.26689765, ...,  0.02839323,\n",
       "        -0.5938099 ,  0.7974989 ],\n",
       "       [-0.7854437 , -0.2994969 ,  0.41027388, ...,  0.5222541 ,\n",
       "        -0.49573544,  0.815075  ]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now 'e' is the embedding of all given words:\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5dd8e",
   "metadata": {},
   "source": [
    "* So from these six words we can figure out that the vectors of the first three words will be similar because the words are for fruits and the vectors of the three last words will be also similar because they're same and the're humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a41f9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9911089]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we'll try cosine similarity to check the vectors similarity, cosine similarity is a way to show how two vectors are \n",
    "# similary? If two vectors are in the ame direction then the cosine similarity will be closed to 1.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([e[0]],[e[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06022fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So next we create a classification model. Threr are two ways of creating the tensorflow model (Sequential & Functional).\n",
    "# Till now we've created 'Sequential Models'. The benefits of using 'Functional Model' is, in this model we'll have non \n",
    "# sequencial type of architechture where we can have two inputs and two outputs and we can build complex model.\n",
    "# So we're going to use 'Functional' approach here. \n",
    "# First we create input layer, first we supply shape then data type which will be string and then the name of the layer.\n",
    "# Next we pass the input layer to pre-process, what we get as result will be a pre-processed text.\n",
    "# Next we supply the pre-processed text into BERT-Encoder. So these will be BERT layers.\n",
    "# Next we create a neural network layers. So in NN we'll use Dropout layer because it help prevent model overfitting.\n",
    "# The 2nd layer is dense which will have only one neuron because it's answering us whether email is spam or not.\n",
    "# Next we construct the model.\n",
    "\n",
    "# Bert layers\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Neural network layers\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# Use inputs and outputs to construct a final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7d492",
   "metadata": {},
   "source": [
    "* To know more about **'Sequencial and Functional Models' in Keras**, follow the bellow link: https://becominghuman.ai/sequential-vs-functional-model-in-keras-20684f766057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcb8dc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_4 (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']                   \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_5 (KerasLayer)     {'default': (None,   109482241   ['keras_layer_4[0][0]',          \n",
      "                                768),                             'keras_layer_4[0][1]',          \n",
      "                                 'encoder_outputs':               'keras_layer_4[0][2]']          \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768)}                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['keras_layer_5[0][13]']         \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# To print the model summary:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a45c9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compile the model:\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "886089bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "35/35 [==============================] - 441s 12s/step - loss: 0.7029 - accuracy: 0.5339 - precision: 0.5341 - recall: 0.5321\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 481s 14s/step - loss: 0.5555 - accuracy: 0.7696 - precision: 0.7568 - recall: 0.7946\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 526s 15s/step - loss: 0.4697 - accuracy: 0.8366 - precision: 0.8256 - recall: 0.8536\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 432s 12s/step - loss: 0.4162 - accuracy: 0.8571 - precision: 0.8460 - recall: 0.8732\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 378s 11s/step - loss: 0.3792 - accuracy: 0.8839 - precision: 0.8620 - recall: 0.9143\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 380s 11s/step - loss: 0.3528 - accuracy: 0.8848 - precision: 0.8748 - recall: 0.8982\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 378s 11s/step - loss: 0.3345 - accuracy: 0.8857 - precision: 0.8711 - recall: 0.9054\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 396s 11s/step - loss: 0.3171 - accuracy: 0.8866 - precision: 0.8832 - recall: 0.8911\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 455s 13s/step - loss: 0.3026 - accuracy: 0.8973 - precision: 0.8816 - recall: 0.9179\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 481s 14s/step - loss: 0.2889 - accuracy: 0.9045 - precision: 0.8967 - recall: 0.9143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x276c4fd0cd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we train a model:\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bfe162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 124s 10s/step - loss: 0.2881 - accuracy: 0.9198 - precision: 0.8985 - recall: 0.9465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28805819153785706,\n",
       " 0.9197860956192017,\n",
       " 0.8984771370887756,\n",
       " 0.9465240836143494]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To evaluate the model performance test data:\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18f49ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 125s 10s/step\n"
     ]
    }
   ],
   "source": [
    "# Next we predict the 'X_test' which is a two dimension array and then we flatten it to become one dimension.\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted = y_predicted.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0da321dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then as sigmoid is a bunch of sigmoid values like 0.7, 0.3, 0.2, 0.9 ... so we said if the value is greater than 0.5 print\n",
    "# 1, in other case print 0.  \n",
    "import numpy as np\n",
    "\n",
    "y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b20441d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[167,  20],\n",
       "       [ 10, 177]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Than we've y_test, so using y_predicted and y_test we can plot a confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "cm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f31f8340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix on a better way:\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f3d43b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.92       187\n",
      "           1       0.90      0.95      0.92       187\n",
      "\n",
      "    accuracy                           0.92       374\n",
      "   macro avg       0.92      0.92      0.92       374\n",
      "weighted avg       0.92      0.92      0.92       374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To print a classification report:\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16e627",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40c52002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7637597 ],\n",
       "       [0.81557024],\n",
       "       [0.7543627 ],\n",
       "       [0.19859484],\n",
       "       [0.09751444]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So here we provide five sentences for prediction:\n",
    "reviews = [\n",
    "    'Enter a chance to win $5000, hurry up, offer valid until march 31, 2021',\n",
    "    'You are awarded a SiPix Digital Camera! call 09061221061 from landline. Delivery within 28days. T Cs Box177. M221BP. 2yr warranty. 150ppm. 16 . p p3.99',\n",
    "    'it to 80488. Your 500 free text messages are valid until 31 December 2005.',\n",
    "    'Hey Sam, Are you coming for a cricket game tomorrow',\n",
    "    \"Why don't you wait 'til at least wednesday to see if you get your .\"\n",
    "]\n",
    "model.predict(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "941d95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From probability we see that the first three sentences are looking to be spam so thats why it's probability is higher than\n",
    "# 0.5%. The last two sentences are looking to be not spam so it's probability is lower than 0.5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
