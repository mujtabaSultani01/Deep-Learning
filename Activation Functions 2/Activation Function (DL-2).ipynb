{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55359ec",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "**Activation function** are used in deep learning to introduce non-linearity into a neural network. They are mathematical equations that determine the output of a neural network based on its input. Examples of activation functions include the sigmoid function, the hyperbolic tangent function, the rectified linear unit (ReLU) function, and the softmax function.\n",
    "* 'Step activation' function is a type of binary activation function which is used in neural networks. It's used as a non-linear thresholding function. This type of activation function takes a single input and produces an output of either 0 or 1 depending on the input value. For example, if the input is greater than the threshold value, then the output is 1; otherwise, the output is 0. \n",
    "* The general guaidline is to use 'sigmoid' function [0, 1] at the output layer and in other layers use 'tanh' function which gives you the range of [-1, 1].\n",
    "* The main issue with 'sigmoid' and 'tanh' function is VANISHING GRADIENT problem.\n",
    "* The next activation function is 'relu'. It says if your value is less than zero then output is zero but if your value is greater than zero then output is same as that value. For hidden layers 'relu' is most popular used function. For hidden layers if you're not sure which activation function should I use, just use 'relu' as your default choice. 'relu' also has VANISHING GRADIENT problem and for that there is another function called 'leaky relu'. 'leaky relu' has less VANISHING GRADIENT problem comparing with 'relu' function.\n",
    "* 'Leaky ReLU' is an extension of the ReLU activation function. It is used in neural networks to help address the problem of “dying neurons”, where a neuron that has become saturated with a large input never recovers. Instead of outputting 0 for any negative input, a leaky ReLU outputs a small negative slope. This allows the neuron to continue to update its weights even with a negative input.\n",
    "* **Which activation function we should use for our problem? sometimes for this question there is not a clear answer. In the output if you have binary classification, you will probably use sigmoid. In the hidden layers you will mostly use relu or leaky relu. We should try out these with ourselves. We try all activation function and see which one give us the best answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ff566",
   "metadata": {},
   "source": [
    "<img src = \"activation_function.png\" width = \"600px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0af7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the function how they convert the input into a specific range. Here we import math model and then use the functions:\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))    # Will return the output of sigmoid function which is in [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc30285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try some values:\n",
    "sigmoid(3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a074151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999982"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 34\n",
    "sigmoid(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579733eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88894836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.357622968839299e-14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also try some negative values.\n",
    "sigmoid(-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a173180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second function is 'tanh':\n",
    "def tanh(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x)) # Will convert a value between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28282e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950547536867306"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's give some values:\n",
    "tanh(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "009ddbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999958776926"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -10\n",
    "tanh(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f728296d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615941559557649"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "tanh(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "915cedfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0\n",
    "tanh(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9927102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7615941559557649"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1\n",
    "tanh(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75b7219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next function is relu:\n",
    "def relu(x):\n",
    "    return max(0, x)    # will return 0 if the value is smaller then 0 and return the value if it's greater then 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a439a6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try some values:\n",
    "relu(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef36a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0\n",
    "relu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f746716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -10\n",
    "relu(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bda26b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "relu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5c287b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1\n",
    "relu(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71a256bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next function is leaky relu:\n",
    "def leaky_relu(x):\n",
    "    return max(0.1*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1388ec43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try some values:\n",
    "leaky_relu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4445fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1\n",
    "leaky_relu(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c754a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100\n",
    "leaky_relu(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e547a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -100\n",
    "leaky_relu(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3e18d",
   "metadata": {},
   "source": [
    "* **These function implementation was just for concept understanding, in real these functions are implemented in tensorflow and keras, we just call them and then they implement their concepts.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
